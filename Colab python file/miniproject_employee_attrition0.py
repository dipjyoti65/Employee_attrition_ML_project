# -*- coding: utf-8 -*-
"""Miniproject_Employee_attrition0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sdbNMFOtiwYurKyDbZHZ_tI42yNSplRG
"""

!pip uninstall scikit-learn -y
!pip install -U scikit-learn
!pip install -U imbalanced-learn

#data cleaning, preprocessing, analysis, import export csv
import pandas as pd
#numpy is used: mathematical function like mean in crossvalidation
import numpy as np
#matplotlib is used for data visualization
from matplotlib import pyplot as plt
#seaborn is used in statistical plot
import seaborn as sns
#undersampling over sampling . imbalance data handling
import imblearn
#svm for suppport vector machine and svc is suppurt vector classification
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_digits
#It is commonly used to encode categorical labels (class labels) into numerical values.
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_selection import mutual_info_classif
digits = load_digits()

import sklearn

sklearn.__version__

from google.colab import drive

from google.colab import drive
drive.mount('/content/drive')

path="/content/drive/MyDrive/mini_project/watson_healthcare_modified.csv"
df=pd.read_csv(path)
df.head(10)

df.shape

df.columns

df.info()

df.isna().sum()

df.duplicated(keep="first").sum()

"""## Exploratory Data Analysis

MonthlyIncome
"""

bins = len(df["MonthlyIncome"].unique())
bins

plt.figure(figsize=(6,6))
sns.histplot(data=df,x=df["MonthlyIncome"],kde=True)
plt.tight_layout()
plt.show()

df["MonthlyRate"].value_counts()

plt.figure(figsize=(6,6))
sns.histplot(data=df,x=df["MonthlyRate"],kde=True)
plt.tight_layout()
plt.show()

"""Varius values"""

df["Shift"].value_counts()

df["NumCompaniesWorked"].value_counts()

df["MaritalStatus"].value_counts()

df["OverTime"].value_counts()

df["TotalWorkingYears"].value_counts()

"""*Number of Job Involvement*"""

df["JobInvolvement"].value_counts()

plt.figure(figsize=(6,6))
sns.countplot(data=df,x="JobInvolvement")
plt.title("Number Of Job Undertaken Rate")
plt.tight_layout()

""" *Total working year*"""

df["TotalWorkingYears"].value_counts()

plt.figure(figsize=(10,10))
sns.histplot(data=df,x="TotalWorkingYears")
plt.title("Total_Working_Years Rate")
plt.tight_layout()

"""*Attrition*"""

df["Attrition"].value_counts()

labels=df["Attrition"].value_counts().index.tolist()
labels

label_to_value = {'No': 0,'Yes': 1}

labels=["Attrition" if i=='Yes' else "No Attrition" for i in label_to_value]
labels

plt.figure(figsize=(6,6))
plt.pie(x=df["Attrition"].value_counts(),labels=labels,autopct="%1.2f%%",explode=[0,0.1])
plt.title("Attrition_rate")
plt.tight_layout()
plt.show()

"""*Years Since Last Promotion*"""

df["YearsSinceLastPromotion"].value_counts()

labels=df["YearsSinceLastPromotion"].value_counts().index.tolist()
labels

plt.figure(figsize=(6,6))
sns.countplot(x="YearsSinceLastPromotion",data=df)
plt.title("Number Of Year Spent Since Last Promotion")
plt.tight_layout()
plt.show()

df["YearsSinceLastPromotion"].mean()

"""*Department*"""

df["Department"].value_counts()

labels=df["Department"].value_counts().index.tolist()
labels

plt.figure(figsize=(6,6))
sns.countplot(x="Department",data=df)
plt.title("Department")
plt.tight_layout()
plt.show()

"""Education"""

df["EducationField"].value_counts()

labels=df["EducationField"].value_counts().index.tolist()
labels

plt.figure(figsize=(6,6))
plt.pie(x=df["EducationField"].value_counts(),labels=labels,autopct="%1.2f%%")
plt.title("EducationField")
plt.tight_layout()
plt.show()

df["JobRole"].value_counts()

df["BusinessTravel"].value_counts()

df["MaritalStatus"].value_counts()

labels=df["MaritalStatus"].value_counts().index.tolist()
labels

plt.figure(figsize=(6,6))
plt.pie(x=df["MaritalStatus"].value_counts(),labels=labels,autopct="%1.2f%%")
plt.title("MaritalStatus")
plt.tight_layout()
plt.show()

df["EnvironmentSatisfaction"].value_counts()

df["NumCompaniesWorked"].value_counts()

labels=df["NumCompaniesWorked"].value_counts().index.tolist()
labels

plt.figure(figsize=(6,6))
plt.pie(x=df["NumCompaniesWorked"].value_counts(),labels=labels,autopct="%1.2f%%")
plt.title("Number of Companies Worked")
plt.tight_layout()
plt.show()

df["PercentSalaryHike"].value_counts()

df["PerformanceRating"].value_counts()

df["RelationshipSatisfaction"].value_counts()

df["StandardHours"].value_counts()

df["Shift"].value_counts()

df["TrainingTimesLastYear"].value_counts()

df["WorkLifeBalance"].value_counts()

df["YearsInCurrentRole"].value_counts()

df["YearsWithCurrManager"].value_counts()

df["Age"].value_counts()

df["DistanceFromHome"].value_counts()

"""# IncomeCategory defined

*In this category i have define monthlyincom into three section low,medium,high*
"""

df["MonthlyRate"].unique()

low_threshold = 6000
high_threshold = 15000
def categorize_income(income):
    if income < low_threshold:
        return 'low'
    elif income < high_threshold:
        return 'medium'
    else:
        return 'high'

df['IncomeCategory'] = df['MonthlyRate'].apply(categorize_income)
df

df["IncomeCategory"].unique()

df["IncomeCategory"].value_counts()

labels=df["IncomeCategory"].value_counts().index.tolist()
labels

plt.figure(figsize=(6,6))
plt.pie(x=df["IncomeCategory"].value_counts(),labels=labels,autopct="%1.2f%%")
plt.title("Salary")
plt.tight_layout()
plt.show()

"""# Data Preprocessing

Categorical encoding
"""

#As these attribute have more then two values so we can not perform label encoding ....that's why we are performing categorical encoding
categorical_cols=["BusinessTravel","Department","EducationField","JobRole","MaritalStatus","IncomeCategory"]
encoded_cols=pd.get_dummies(df[categorical_cols],prefix="cat")

encoded_cols

#joinign encoded_cols variable with dataframe
df=df.join(encoded_cols)

df.head()

"""label encoding for two value attribute"""

#import label encoder
# label_encoder object knows
# how to understand word labels.
label_encoder=LabelEncoder()
LabelEncoder()

#label encoding of different column
df["Attrition"]=label_encoder.fit_transform(df["Attrition"])
df["Gender"]=label_encoder.fit_transform(df["Gender"])
df["OverTime"]=label_encoder.fit_transform(df["OverTime"])

"""Droping columns"""

# Droping columns which are defined by categorical encoding
df.drop(["BusinessTravel","Department","EducationField","JobRole","MaritalStatus","IncomeCategory"],inplace=True,axis="columns")

#Droping unnecssary columns
df.drop(["DailyRate","Education","EmployeeCount","HourlyRate","MonthlyRate","MonthlyIncome","Over18"],inplace=True,axis="columns")

df.drop(["StandardHours"],inplace=True,axis="columns")

pd.set_option('display.max_columns', None)

df.head()

"""# Slipt data into train and test"""

# First of all droping dependept variable "Attribute" and saving instance of df as X
# By doing this we are seperating dependept and independent columns
X=df.drop("Attrition",axis=1)
# Here we are saving dependept variable "Attribute" to Y
Y=df["Attrition"]

"""Random Undersampling"""

# Random Undersampling is performed as our dataset is imbalanced .... and model becomes biased towards majority class
from imblearn.under_sampling import RandomUnderSampler
ros=RandomUnderSampler(sampling_strategy=.5)
X_res,Y_res= ros.fit_resample(X,Y)
ax = Y_res.value_counts().plot.pie(autopct='%.2f')
_ = ax.set_title("Under_sampling")

#added by suhel
df.Attrition.value_counts()

X_res.shape

Y_res.shape

X.head()

Y.head()

#creating training and testing data
from sklearn.model_selection import train_test_split
X_train, X_test , Y_train , Y_test = train_test_split(X,Y, train_size=0.8)

X_train.shape

X_test.shape

X_test.head()

Y_test.head()

"""## Feature selection -Information gain - mutual information in classification"""

from sklearn.feature_selection import mutual_info_classif
# determine the mutual information
mutual_info = mutual_info_classif(X_res,Y_res)
mutual_info

mutual_info = pd.Series(mutual_info)
mutual_info.index = X_res.columns
mutual_info.sort_values(ascending=False)

# Let's plot the ordered mutual__info values per feature
mutual_info.sort_values(ascending=False).plot.bar(figsize=(20,8))

from sklearn.feature_selection import SelectKBest

"""## Select K best"""

# # Now we will select the top 8 important features
sel_twelve_cols = SelectKBest(mutual_info_classif,k=15)
sel_twelve_cols.fit(X_res,Y_res)
X_res.columns[sel_twelve_cols.get_support()]

# Saving selected features to a new variable for training the model
selected_features=X_res[X_res.columns[sel_twelve_cols.get_support()]]
# Saving selected featues to a new variable for testing the model
test_features=X_test[X_res.columns[sel_twelve_cols.get_support()]]

selected_features.columns

"""# Showing how selected features affect Attrition"""

continuous_attributes = ['Age', 'JobLevel', 'OverTime', 'Shift', 'TotalWorkingYears',
       'YearsAtCompany', 'YearsInCurrentRole', 'YearsWithCurrManager']


for attribute in continuous_attributes:
    plt.figure(figsize=(12, 6))
    sns.countplot(data=df, x=attribute, hue='Attrition')
    plt.title(f'{attribute} vs. Attrition')
    plt.xlabel(attribute)
    plt.ylabel('Count')
    plt.legend(title='Attrition', labels=['No', 'Yes'])
    plt.show()

"""## Train model

Logistic Regression Model
"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score,classification_report , confusion_matrix

logReg=LogisticRegression(max_iter=20000)

logReg.fit(selected_features,Y_res)

logReg_predictions=logReg.predict(test_features)

accuracy_score(Y_test,logReg_predictions)

print(classification_report(Y_test,logReg_predictions ))

cm=confusion_matrix(Y_test,logReg_predictions)

cm

plt.figure(figsize=(10,6))
sns.heatmap(cm,annot=True)
plt.tight_layout()
plt.show()

"""Random Forest"""

from sklearn.ensemble import RandomForestClassifier

ranForest = RandomForestClassifier(n_estimators=100)

ranForest.fit(selected_features,Y_res)

ranForest_precdictions= ranForest.predict(test_features)

accuracy_score(Y_test,ranForest_precdictions)

print(classification_report(Y_test,ranForest_precdictions))

cm = confusion_matrix(Y_test,ranForest_precdictions)

cm

plt.figure(figsize=(10,6))
sns.heatmap(cm,annot=True)
plt.tight_layout()
plt.show()

from sklearn.tree import plot_tree
from sklearn.datasets import make_classification
import matplotlib.pyplot as plt

# newly added by suhel. Marking if anything goes wrong

# test_features.columns

#added by suhel
#data=[[27,1,3,1,1,5,3,2,2,2,1,1,0,1,1]]
#new_df =pd.DataFrame(data,columns=['Age', 'EnvironmentSatisfaction', 'JobLevel', 'OverTime', 'Shift','TotalWorkingYears', 'YearsAtCompany', 'YearsInCurrentRole','YearsSinceLastPromotion', 'YearsWithCurrManager', 'cat_Non-Travel','cat_Admin', 'cat_Administrative', 'cat_Other', 'cat_Single'])

#added by suhel
#probability

##added by suhel
# probability = ranForest.predict_proba(new_df)[:,1]

"""Support vector machine"""

svm=SVC()
svm.fit(selected_features,Y_res)

svm_predictions=svm.predict(test_features)

accuracy_score(Y_test,svm_predictions)

print(classification_report(Y_test,svm_predictions))

cm=confusion_matrix(Y_test,svm_predictions)
cm

plt.figure(figsize=(10,6))
sns.heatmap(cm,annot=True)
plt.tight_layout()
plt.show()

"""# K-fold Crossvalidation"""

from sklearn.model_selection import StratifiedKFold
folds = StratifiedKFold(n_splits=3)

from sklearn.model_selection import cross_val_score

"""Crossvalidation for LogisticRegression"""

cross_val_score(LogisticRegression(max_iter=20000),digits.data,digits.target)

scores_log=cross_val_score(logReg,selected_features,Y_res,cv=10)
scores_log

np.mean(scores_log)

scores_log_test=cross_val_score(logReg,test_features,Y_test,cv=10)
scores_log_test

np.mean(scores_log_test)

"""Crossvalidation for RandomForestClassificatin"""

scores_rand=cross_val_score(RandomForestClassifier(n_estimators=100),digits.data,digits.target)
scores_rand

np.mean(scores_rand)

scores=cross_val_score(ranForest,selected_features,Y_res,cv=10)
scores

np.mean(scores)

scores_test=cross_val_score(ranForest,test_features,Y_test,cv=10)
scores_test

np.mean(scores_test)

""" Support Vector Machine"""

cross_val_score(SVC(),digits.data,digits.target)

scores_svm=cross_val_score(svm,selected_features,Y_res,cv=10)
scores

np.mean(scores)

scores_test=cross_val_score(svm,test_features,Y_test,cv=10)
scores_test

np.mean(scores_test)

"""# Saving the Random forest model"""

import joblib

joblib.dump(ranForest,"Random_forest_model.joblib")

# load_ranforest= joblib.load('/content/Random_forest_model')

#load_ranforest.predict(test_features)

import pickle

#pickle.dump(ranForest, open('/content/employee_attrition_randomforest_model','wb'))

from google.colab import files

with open('randomforest_modelY.pkl', 'wb') as file:
    pickle.dump(ranForest, file)

files.download('randomforest_modelY.pkl')